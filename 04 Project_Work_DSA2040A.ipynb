{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9099885d-579f-4624-85f5-1d2fbe3c00bd",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;\">\n",
    "<b>COMPREHENSIVE RETAIL ANALYTICS THROUGH PRODUCT AND CUSTOMER SEGMENTATION</b>\n",
    "</h2>\n",
    "\n",
    "<h4 style=\"text-align:center;\">\n",
    "<i>Author: Isaiah Akuku</i>\n",
    "</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ba0a25-0a4c-4ef3-9b55-cb614fd039ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Jupyter core packages...\n",
      "IPython          : 8.20.0\n",
      "ipykernel        : 6.28.0\n",
      "ipywidgets       : not installed\n",
      "jupyter_client   : 8.6.0\n",
      "jupyter_core     : 5.5.0\n",
      "jupyter_server   : 2.10.0\n",
      "jupyterlab       : 4.0.13\n",
      "nbclient         : 0.8.0\n",
      "nbconvert        : 7.10.0\n",
      "nbformat         : 5.9.2\n",
      "notebook         : 7.0.8\n",
      "qtconsole        : not installed\n",
      "traitlets        : 5.7.1\n"
     ]
    }
   ],
   "source": [
    "#Check version\n",
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b57929-6024-4ec9-8586-a244cc9a3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pickleshare\n",
    "#pip install pygments\n",
    "#pip install python-docx\n",
    "#pip install pandas matplotlib seaborn python-docx\n",
    "#pip install notebook\n",
    "#pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b2b0b48-93a9-41b6-a80c-5c1ef673b6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Documents\\DSA\\Spring 2025\\DSA2040A\\Assignment\\Group Project\n"
     ]
    }
   ],
   "source": [
    "#Set directory\n",
    "#%cd \"C:/Users/ADMIN/Documents/DSA/Spring 2025/DSA2040A/Assignment/Group Project/Restaurant Data\"\n",
    "%cd \"C:/Users/ADMIN/Documents/DSA/Spring 2025/DSA2040A/Assignment/Group Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca32bbc6-0bff-4bf4-8bf0-ee96775bb9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31835c4-dc14-4995-b2bd-3f1e298568bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Cleaning ===\n",
      "Initial records: 541910\n",
      "\n",
      "=== Product Clustering ===\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "# TASK A: Comprehensive retail analytics framework\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Task 0. Environment setup\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.metrics import accuracy_score, silhouette_score\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "# Configuration\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10\n",
    "})\n",
    "sns.set_palette(\"husl\")\n",
    "os.makedirs('04 visualizations', exist_ok=True)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "# Task 1. Data loading and preparation\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "def load_and_clean_data():\n",
    "    \"\"\"Load and clean the retail transaction data\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel('retail_dataset.xlsx',\n",
    "                           usecols=['Invoice', 'StockCode', 'Description',\n",
    "                                    'Quantity', 'InvoiceDate', 'Price', 'Country', 'Customer ID'],\n",
    "                           parse_dates=['InvoiceDate'],\n",
    "                           engine='openpyxl')\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Data file not found. Please ensure 'retail_dataset.xlsx' exists.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={\n",
    "        'Invoice': 'InvoiceNo',\n",
    "        'Price': 'UnitPrice',\n",
    "        'Customer ID': 'CustomerID'\n",
    "    })\n",
    "\n",
    "    print(\"\\n=== Data Cleaning ===\")\n",
    "    print(\"Initial records:\", len(df))\n",
    "\n",
    "    # Clean product descriptions\n",
    "    df['Clean_Description'] = df['Description'].str.strip().str.lower()\n",
    "    mask = df['Clean_Description'].str.contains(r'^\\?+.*|\\?+.*\\?+$|^\\?+$', na=False)\n",
    "    df = df[~mask]\n",
    "    df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "\n",
    "    # Create categories\n",
    "    df['Price_Cat'] = pd.cut(df['UnitPrice'], \n",
    "                            bins=[-np.inf, 5, 10, 50, np.inf],\n",
    "                            labels=['Budget', 'Mid-Range', 'Premium', 'Luxury'])\n",
    "    df['Quantity_Cat'] = pd.cut(df['Quantity'].abs(),\n",
    "                               bins=[-np.inf, 5, 10, 50, np.inf],\n",
    "                               labels=['Single', 'Multi', 'Bulk', 'Wholesale'])\n",
    "\n",
    "    return df.dropna(subset=['Clean_Description']).reset_index(drop=True)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "# Task 2: Product Clustering\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "def cluster_products(df):\n",
    "    \"\"\"Cluster products using TF-IDF and KMeans\"\"\"\n",
    "    print(\"\\n=== Product Clustering ===\")\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    tfidf = TfidfVectorizer(max_df=0.85, min_df=5, stop_words='english',\n",
    "                           ngram_range=(1,2), max_features=5000)\n",
    "    tfidf_matrix = tfidf.fit_transform(df['Clean_Description'])\n",
    "\n",
    "    # Cluster optimization\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    k_range = range(3, 8)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=2040, n_init=10)\n",
    "        labels = kmeans.fit_predict(tfidf_matrix)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(tfidf_matrix, labels))\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(k_range, inertias, marker='o')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(k_range, silhouette_scores, marker='o', color='orange')\n",
    "    plt.title('Silhouette Scores')\n",
    "    plt.savefig('04 visualizations/clustering_metrics.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Final clustering\n",
    "    optimal_k = 6\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=2040, n_init=20)\n",
    "    df['Product_Cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
    "    \n",
    "    # Cluster labeling\n",
    "    cluster_labels = {}\n",
    "    for cluster_id in range(optimal_k):\n",
    "        text = ' '.join(df[df['Product_Cluster'] == cluster_id]['Clean_Description'].sample(500, replace=True))\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.title(f\"Cluster {cluster_id} Keywords\")\n",
    "        plt.axis('off')\n",
    "        plt.savefig(f'04 visualizations/cluster_{cluster_id}_keywords.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        cluster_labels[cluster_id] = ', '.join(list(wordcloud.words_.keys())[:5])\n",
    "    \n",
    "    df['Cluster_Label'] = df['Product_Cluster'].map(cluster_labels)\n",
    "    \n",
    "    # PCA Visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_features = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    df['PCA1'] = reduced_features[:,0]\n",
    "    df['PCA2'] = reduced_features[:,1]\n",
    "    \n",
    "    plt.figure(figsize=(14,8))\n",
    "    sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster_Label', \n",
    "                   data=df, palette='tab10', alpha=0.7)\n",
    "    plt.title('Product Cluster Visualization')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.savefig('04 visualizations/product_clusters_pca.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "# Task 3: Association Rule Mining\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "def generate_association_rules(df):\n",
    "    \"\"\"Generate association rules using FP-growth\"\"\"\n",
    "    print(\"\\n=== Market Basket Analysis ===\")\n",
    "    \n",
    "    # Create basket matrix\n",
    "    basket = (df.groupby(['InvoiceNo', 'Cluster_Label'])['Quantity']\n",
    "             .sum().unstack().fillna(0)\n",
    "             .applymap(lambda x: 1 if x > 0 else 0))\n",
    "\n",
    "    # FP-growth implementation\n",
    "    frequent_itemsets = fpgrowth(basket, min_support=0.03, use_colnames=True)\n",
    "    \n",
    "    if not frequent_itemsets.empty:\n",
    "        rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "        rules = rules.sort_values('confidence', ascending=False)\n",
    "        \n",
    "        # Rule visualizations\n",
    "        plt.figure(figsize=(12,6))\n",
    "        top_rules = rules.head(15).sort_values('confidence', ascending=True)\n",
    "        plt.barh(y=range(len(top_rules)), width=top_rules['confidence'], \n",
    "                color=sns.color_palette(\"viridis\", len(top_rules)))\n",
    "        plt.yticks(range(len(top_rules)), [f\"{a} → {c}\" for a,c in zip(top_rules['antecedents'], top_rules['consequents'])])\n",
    "        plt.title('Top 15 Association Rules by Confidence')\n",
    "        plt.savefig('04 visualizations/top_rules.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Support vs Lift\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.scatterplot(data=rules, x='support', y='lift', hue='confidence',\n",
    "                       palette='viridis', sizes=(20, 200))\n",
    "        plt.title('Support vs Lift')\n",
    "        plt.savefig('04 visualizations/support_lift_confidence.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Network graph\n",
    "        G = nx.DiGraph()\n",
    "        top_rules_graph = rules.sort_values('lift', ascending=False).head(30)\n",
    "        for _, row in top_rules_graph.iterrows():\n",
    "            antecedent = list(row['antecedents'])\n",
    "            consequent = list(row['consequents'])\n",
    "            G.add_edge(', '.join(antecedent), ', '.join(consequent), \n",
    "                      label=f\"Conf: {row['confidence']:.2f}\")\n",
    "        \n",
    "        plt.figure(figsize=(15,10))\n",
    "        pos = nx.spring_layout(G)\n",
    "        nx.draw(G, pos, with_labels=True, node_size=2000, \n",
    "               font_size=8, arrowsize=20)\n",
    "        plt.title('Association Rule Network')\n",
    "        plt.savefig('04 visualizations/rules_network.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        return rules\n",
    "    return pd.DataFrame()\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "# Task 4: Customer Segmentation\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "def perform_customer_segmentation(df):\n",
    "    \"\"\"Segment customers using purchasing behavior\"\"\"\n",
    "    print(\"\\n=== Customer Segmentation ===\")\n",
    "    \n",
    "    customer_features = df.groupby('CustomerID').agg({\n",
    "        'UnitPrice': 'mean',\n",
    "        'Quantity': 'sum',\n",
    "        'InvoiceNo': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    processed_data = scaler.fit_transform(customer_features[['UnitPrice', 'Quantity', 'InvoiceNo']])\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=4, random_state=2040, n_init=20)\n",
    "    customer_features['Segment'] = kmeans.fit_predict(processed_data)\n",
    "    \n",
    "    # PCA Visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(processed_data)\n",
    "    customer_features[['PCA1', 'PCA2']] = reduced_data\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.scatterplot(x='PCA1', y='PCA2', hue='Segment',\n",
    "                   data=customer_features, palette='Set2', s=100)\n",
    "    plt.title('Customer Segmentation')\n",
    "    plt.savefig('04 visualizations/customer_segments.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return customer_features\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "# Task 5: Report Generation\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "class RetailReportGenerator:\n",
    "    def __init__(self):\n",
    "        self.doc = Document()\n",
    "        self._add_title_page()\n",
    "    \n",
    "    def _add_title_page(self):\n",
    "        self.doc.add_heading('Retail Analytics Report', 0)\n",
    "        self.doc.add_paragraph(f\"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "        self.doc.add_page_break()\n",
    "    \n",
    "    def add_section(self, title, content, images=None):\n",
    "        self.doc.add_heading(title, level=1)\n",
    "        self.doc.add_paragraph(content)\n",
    "        if images:\n",
    "            for img_path in images:\n",
    "                if os.path.exists(img_path):\n",
    "                    self.doc.add_picture(img_path, width=Inches(6))\n",
    "        self.doc.add_page_break()\n",
    "    \n",
    "    def generate_report(self, df, rules, segments):\n",
    "        # Data Overview\n",
    "        self.add_section(\n",
    "            \"Data Overview\",\n",
    "            f\"Analyzed {len(df):,} transactions with {df['StockCode'].nunique():,} unique products\",\n",
    "            ['04 visualizations/clustering_metrics.png']\n",
    "        )\n",
    "        \n",
    "        # Product Analysis\n",
    "        cluster_images = [f'04 visualizations/cluster_{i}_keywords.png' for i in range(6)]\n",
    "        self.add_section(\n",
    "            \"Product Clustering\",\n",
    "            \"Product groups identified through text analysis:\",\n",
    "            cluster_images + ['04 visualizations/product_clusters_pca.png']\n",
    "        )\n",
    "        \n",
    "        # Market Basket Analysis\n",
    "        if not rules.empty:\n",
    "            self.add_section(\n",
    "                \"Association Rules\",\n",
    "                \"Strong product associations identified:\",\n",
    "                ['04 visualizations/top_rules.png',\n",
    "                 '04 visualizations/support_lift_confidence.png',\n",
    "                 '04 visualizations/rules_network.png']\n",
    "            )\n",
    "        \n",
    "        # Customer Analysis\n",
    "        self.add_section(\n",
    "            \"Customer Segmentation\",\n",
    "            \"Customer groups based on purchasing behavior:\",\n",
    "            ['04 visualizations/customer_segments.png']\n",
    "        )\n",
    "        \n",
    "        self.doc.save('comprehensive_retail_report.docx')\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "# Main Execution\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Data pipeline\n",
    "    df = load_and_clean_data()\n",
    "    df = cluster_products(df)\n",
    "    \n",
    "    # Analytics pipeline\n",
    "    rules = generate_association_rules(df)\n",
    "    customer_segments = perform_customer_segmentation(df)\n",
    "    \n",
    "    # Reporting\n",
    "    report = RetailReportGenerator()\n",
    "    report.generate_report(df, rules, customer_segments)\n",
    "    \n",
    "    # Save outputs\n",
    "    df.to_csv('clustered_products.csv', index=False)\n",
    "    customer_segments.to_csv('customer_segments.csv', index=False)\n",
    "    \n",
    "    print(\"\\n✅ Analysis Complete\")\n",
    "    print(\"Generated Files:\")\n",
    "    print(\"- comprehensive_retail_report.docx\")\n",
    "    print(\"- 04 visualizations/ [All analysis charts]\")\n",
    "    print(\"- clustered_products.csv\")\n",
    "    print(\"- customer_segments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b27d6b3-4646-475d-8298-4595b7e0e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Task B: Make decision tree from the clustered descriptions\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "# Setup\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "os.makedirs('04 visualizations', exist_ok=True)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "# 1. Load and prepare data\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "df = pd.read_excel(\"description_retail.xlsx\")\n",
    "df = df.dropna(subset=['Description'])\n",
    "df['Description'] = df['Description'].astype(str).str.lower().str.strip()\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Task 2. TF-IDF Vectorization\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "tfidf = TfidfVectorizer(max_df=0.8, min_df=5, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['Description'])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Task3. KMeans Clustering\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "k = 6\n",
    "model = KMeans(n_clusters=k, random_state=2040)\n",
    "df['cluster'] = model.fit_predict(tfidf_matrix)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Task 4. Labeling Clusters\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "cluster_labels = {\n",
    "    0: \"Gift Sets & Stationery\",\n",
    "    1: \"Accessories & Decor\",\n",
    "    2: \"Seasonal Holiday Items\",\n",
    "    3: \"Bags & Gifting Items\",\n",
    "    4: \"Candle Holders & Lighting\",\n",
    "    5: \"Boxes & Storage Items\"\n",
    "}\n",
    "df['Cluster_Label'] = df['cluster'].map(cluster_labels)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Task 5. Dimensionality reduction for plot\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(tfidf_matrix.toarray())\n",
    "df['PCA1'] = reduced[:, 0]\n",
    "df['PCA2'] = reduced[:, 1]\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Task 6. Visualization\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x='PCA1', y='PCA2',\n",
    "    hue='Cluster_Label',\n",
    "    data=df,\n",
    "    palette='tab10',\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(\"Item description clusters\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"04 visualizations/item_clusters_named.png\")\n",
    "plt.close()\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Task 7. Load retail dataset and merge\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "retail_df = pd.read_excel(\"retail_dataset.xlsx\")\n",
    "retail_df = retail_df.rename(columns={\n",
    "    'Invoice': 'InvoiceNo',\n",
    "    'Price': 'UnitPrice',\n",
    "    'Customer ID': 'CustomerID'\n",
    "})\n",
    "retail_df['Description'] = retail_df['Description'].astype(str).str.lower().str.strip()\n",
    "merged_df = pd.merge(retail_df, df[['Description', 'Cluster_Label']], on='Description', how='left')\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Task 8. Create price category and ensure quantity is positive\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "merged_df['Quantity'] = merged_df['Quantity'].abs()\n",
    "merged_df['Price_Cat'] = pd.cut(merged_df['UnitPrice'], \n",
    "                                bins=[-float('inf'), 5, 10, 50, float('inf')],\n",
    "                                labels=['Budget', 'Mid-Range', 'Premium', 'Luxury'])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Task 9. Decision tree to predict price catgory\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "merged_df = merged_df.dropna(subset=['Cluster_Label', 'Price_Cat', 'Country'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "X = merged_df[['Country', 'Cluster_Label', 'Quantity']].copy()\n",
    "X['Country'] = le.fit_transform(X['Country'])\n",
    "X['Cluster_Label'] = le.fit_transform(X['Cluster_Label'])\n",
    "y = le.fit_transform(merged_df['Price_Cat'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2040)\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=2040)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Task 10. Save decision tree visualization\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(clf, filled=True,\n",
    "          feature_names=X.columns.tolist(),\n",
    "          class_names=le.classes_)\n",
    "plt.title(f'Price Category Prediction Tree (Accuracy: {accuracy:.2%})')\n",
    "plt.savefig('04 visualizations/decision_tree_price_cat.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#Task 11. Save final merged dataset\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "merged_df.to_excel(\"04 visualizations/retail_with_clusters.xlsx\", index=False)\n",
    "\n",
    "accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
